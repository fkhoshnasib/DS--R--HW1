{
  "cells": [
    {
      "metadata": {
        "id": "6ac74d2c4769e77a"
      },
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/thesteve0/impatient-computer-vision/blob/main/2_classify_embed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Classification and Embedding\n",
        "\n",
        "We are going to do our housekeep steps which will take a little while to run. While they are running we will go back to slides and I will introduce the topics.\n",
        "\n",
        "### Housekeeping\n",
        "Before we do anything else, we are need to change our machine time to one that has a GPU. Doing computer vision tasks with a CPU, except for some specific models, is extremely slow. One of the reasons we are using Colab is that you can get free access to a GPU for the workshop.\n",
        "\n",
        "**FROM THIS NOTEBOOK FORWARD YOU WILL NEED TO CONNECT TO THIS RUNTIME TYPE**\n",
        "\n",
        "Please:\n",
        "1. Go up to the top right of the browser\n",
        "2. Select \"Connect\"\n",
        "3. Then \"Change Runtime Type\"\n",
        "![change_runtime](https://github.com/thesteve0/impatient-computer-vision/blob/main/assets/2_pick_GPU1.png?raw=1)\n",
        "\n",
        "4. Pick T4 GPU\n",
        "5. Click Save\n",
        "![pick GPU](https://github.com/thesteve0/impatient-computer-vision/blob/main/assets/2_pick_GPU2.png?raw=1)\n",
        "\n",
        "6. When the runtime connects, it should look like this\n",
        "![running GPU](https://github.com/thesteve0/impatient-computer-vision/blob/main/assets/2_pick_GPU3.png?raw=1)\n",
        "\n",
        "\n",
        "Now Time to do our long running tasks\n",
        "1. Map the drive\n",
        "2. Load the dependencies\n",
        "3. Load the data"
      ],
      "id": "6ac74d2c4769e77a"
    },
    {
      "metadata": {
        "id": "d62882049a2cb251",
        "outputId": "3d852474-c46d-443c-b1d3-c21c79a19048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting fiftyone==1.4.1\n",
            "  Downloading fiftyone-1.4.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Collecting aiofiles (from fiftyone==1.4.1)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting argcomplete (from fiftyone==1.4.1)\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (4.13.4)\n",
            "Collecting boto3 (from fiftyone==1.4.1)\n",
            "  Downloading boto3-1.38.14-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (5.5.2)\n",
            "Collecting dacite<1.8.0,>=1.6.0 (from fiftyone==1.4.1)\n",
            "  Downloading dacite-1.7.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (1.2.18)\n",
            "Collecting ftfy (from fiftyone==1.4.1)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (4.12.3)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone==1.4.1)\n",
            "  Downloading hypercorn-0.17.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (3.1.6)\n",
            "Collecting kaleido!=0.2.1.post1 (from fiftyone==1.4.1)\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (3.10.0)\n",
            "Collecting mongoengine~=0.29.1 (from fiftyone==1.4.1)\n",
            "  Downloading mongoengine-0.29.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting motor~=3.6.0 (from fiftyone==1.4.1)\n",
            "  Downloading motor-3.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (2.2.2)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (11.2.1)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (5.24.1)\n",
            "Collecting pprintpp (from fiftyone==1.4.1)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (5.9.5)\n",
            "Collecting pymongo~=4.9.2 (from fiftyone==1.4.1)\n",
            "  Downloading pymongo-4.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (2025.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (2024.11.6)\n",
            "Collecting retrying (from fiftyone==1.4.1)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting rtree (from fiftyone==1.4.1)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (1.15.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (75.2.0)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone==1.4.1)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone==1.4.1)\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting starlette>=0.24.0 (from fiftyone==1.4.1)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting strawberry-graphql (from fiftyone==1.4.1)\n",
            "  Downloading strawberry_graphql-0.268.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (0.9.0)\n",
            "Collecting xmltodict (from fiftyone==1.4.1)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone==1.4.1)\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting pydash (from fiftyone==1.4.1)\n",
            "  Downloading pydash-8.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting fiftyone-brain<0.21,>=0.20.1 (from fiftyone==1.4.1)\n",
            "  Downloading fiftyone_brain-0.20.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting fiftyone-db<2.0,>=0.4 (from fiftyone==1.4.1)\n",
            "  Downloading fiftyone_db-1.1.7.tar.gz (7.9 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting voxel51-eta<0.15,>=0.14.0 (from fiftyone==1.4.1)\n",
            "  Downloading voxel51_eta-0.14.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from fiftyone==1.4.1) (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: h11 in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone==1.4.1) (0.16.0)\n",
            "Requirement already satisfied: h2>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone==1.4.1) (4.2.0)\n",
            "Collecting priority (from hypercorn>=0.13.2->fiftyone==1.4.1)\n",
            "  Downloading priority-2.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone==1.4.1)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3->fiftyone==1.4.1) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.14->fiftyone==1.4.1) (9.1.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo~=4.9.2->fiftyone==1.4.1)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fiftyone==1.4.1) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette>=0.24.0->fiftyone==1.4.1) (4.9.0)\n",
            "Requirement already satisfied: httpx>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone==1.4.1) (0.28.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (0.3.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (1.0.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (2.9.0.post0)\n",
            "Collecting rarfile (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (1.17.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (5.3.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->fiftyone==1.4.1) (2.7)\n",
            "Collecting botocore<1.39.0,>=1.38.14 (from boto3->fiftyone==1.4.1)\n",
            "  Downloading botocore-1.38.14-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone==1.4.1)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.13.0,>=0.12.0 (from boto3->fiftyone==1.4.1)\n",
            "  Downloading s3transfer-0.12.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->fiftyone==1.4.1) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->fiftyone==1.4.1) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone==1.4.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone==1.4.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone==1.4.1) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone==1.4.1) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone==1.4.1) (3.2.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fiftyone==1.4.1) (2025.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone==1.4.1) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone==1.4.1) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone==1.4.1) (0.4)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql->fiftyone==1.4.1)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone==1.4.1) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone==1.4.1) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone==1.4.1) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone==1.4.1) (4.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone==1.4.1) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone==1.4.1) (1.0.9)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (25.3.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (3.22.0)\n",
            "Collecting pyzstd>=0.15.9 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading pyppmd-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading inflate64-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting brotli>=1.1.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->voxel51-eta<0.15,>=0.14.0->fiftyone==1.4.1) (3.4.2)\n",
            "Downloading fiftyone-1.4.1-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dacite-1.7.0-py3-none-any.whl (12 kB)\n",
            "Downloading fiftyone_brain-0.20.1-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.9/110.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypercorn-0.17.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mongoengine-0.29.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading motor-3.6.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Downloading voxel51_eta-0.14.0-py2.py3-none-any.whl (942 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.0/943.0 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.38.14-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.268.1-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading botocore-1.38.14-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Downloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: fiftyone-db\n",
            "  Building wheel for fiftyone-db (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fiftyone-db: filename=fiftyone_db-1.1.7-py3-none-manylinux1_x86_64.whl size=42156243 sha256=081c8644dbe2bd92d8ae89bd87b6c107abd01f3fbb5451df88d9008904cdaf52\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/75/da/54cd52c92db9f0c4e88ca5efdbac2304b49927249c9c859b8d\n",
            "Successfully built fiftyone-db\n",
            "Installing collected packages: texttable, sseclient-py, pprintpp, kaleido, brotli, xmltodict, wsproto, rtree, retrying, rarfile, pyzstd, pyppmd, pydash, pybcj, priority, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multivolumefile, jsonlines, jmespath, inflate64, graphql-core, ftfy, fiftyone-db, dnspython, dacite, argcomplete, aiofiles, strawberry-graphql, starlette, pymongo, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hypercorn, botocore, voxel51-eta, universal-analytics-python3, sse-starlette, s3transfer, nvidia-cusolver-cu12, motor, mongoengine, fiftyone-brain, boto3, fiftyone\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-24.1.0 argcomplete-3.6.2 boto3-1.38.14 botocore-1.38.14 brotli-1.1.0 dacite-1.7.0 dnspython-2.7.0 fiftyone-1.4.1 fiftyone-brain-0.20.1 fiftyone-db-1.1.7 ftfy-6.3.1 graphql-core-3.2.6 hypercorn-0.17.3 inflate64-1.0.1 jmespath-1.0.1 jsonlines-4.0.0 kaleido-0.2.1 mongoengine-0.29.1 motor-3.6.1 multivolumefile-0.2.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pprintpp-0.4.0 priority-2.0.0 py7zr-0.22.0 pybcj-1.0.6 pydash-8.0.5 pymongo-4.9.2 pyppmd-1.1.1 pyzstd-0.17.0 rarfile-4.2 retrying-1.3.4 rtree-1.4.0 s3transfer-0.12.0 sse-starlette-0.10.3 sseclient-py-1.8.0 starlette-0.46.2 strawberry-graphql-0.268.1 texttable-1.7.0 universal-analytics-python3-1.1.1 voxel51-eta-0.14.0 wsproto-1.2.0 xmltodict-0.14.2\n",
            "Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.data.importers:Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 300/300 [17.3ms elapsed, 0s remaining, 17.3K samples/s]     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 300/300 [17.3ms elapsed, 0s remaining, 17.3K samples/s]     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:        our-photos\n",
            "Media type:  image\n",
            "Num samples: 300\n",
            "Persistent:  False\n",
            "Tags:        []\n",
            "Sample fields:\n",
            "    id:               fiftyone.core.fields.ObjectIdField\n",
            "    filepath:         fiftyone.core.fields.StringField\n",
            "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
            "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
            "    created_at:       fiftyone.core.fields.DateTimeField\n",
            "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
            "    open_clip_embed:  fiftyone.core.fields.VectorField\n",
            "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install fiftyone==1.4.1 torch torchvision umap-learn\n",
        "\n",
        "\n",
        "import fiftyone as fo\n",
        "\n",
        "name = \"our-photos\"\n",
        "dir = \"/content/drive/MyDrive/impatient-cv/flickr-labeled\"\n",
        "\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=dir,\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    name=name\n",
        ")\n",
        "\n",
        "print(dataset)"
      ],
      "id": "d62882049a2cb251"
    },
    {
      "metadata": {
        "id": "10bb2367138ac9fd"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification\n",
        "\n",
        "As we discussed in the slides, Classification is the computer vision task where you try to assign an image to single class out of a list of classes. We are going to use a classification model that is the foundation for many other models and is still quite powerful - ResNet. We are going to use the simplement version, ResNet18, because:\n",
        "\n",
        "1. It doesn't require much GPU resources\n",
        "2. It is fast to compute\n",
        "\n",
        "There are many variations to ResNet where a number is appended to the name. This number usually represents the number of layers in the neural network.\n",
        "\n",
        "### Training data\n",
        "\n",
        "While ResNet18 has a specific architecture, to use it for predictions, the model needs to be trained on data. There are many foundational data sets in computer vision but, a partciularly common one is [ImageNet](https://www.image-net.org/index.php). This dataset has 1k classes and millions of annotated images.\n",
        "\n",
        "Please open the list of the [imagenet classes](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/) in another browser tab. We will be referring to this later in the notebook\n",
        "\n",
        "FiftyOne has a [dataset zoo](https://docs.voxel51.com/dataset_zoo/datasets.html) where many important computer vision datasets have been converted into FiftyOne format and are easy to download and view.\n",
        "\n",
        "Let's go ahead and download and view a small subset of the ImageNet Data, the [ImageNet Sample Data](https://docs.voxel51.com/dataset_zoo/datasets.html#imagenet-sample)"
      ],
      "id": "10bb2367138ac9fd"
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "id": "f5b8e88ab46b2d09",
        "outputId": "ef24d87c-b4c5-4529-bb76-6b6eb06587a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "cell_type": "code",
      "source": [
        "import fiftyone.zoo as foz\n",
        "\n",
        "imagenet_samples = foz.load_zoo_dataset(\"imagenet-sample\")\n",
        "\n",
        "session = fo.launch_app(imagenet_samples, auto=False)\n",
        "\n",
        "session.url\n"
      ],
      "id": "f5b8e88ab46b2d09",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset to '/root/fiftyone/imagenet-sample'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading dataset to '/root/fiftyone/imagenet-sample'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets.base:Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |████|  762.4Mb/762.4Mb [730.5ms elapsed, 0s remaining, 1.0Gb/s]        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |████|  762.4Mb/762.4Mb [730.5ms elapsed, 0s remaining, 1.0Gb/s]        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets.base:Extracting dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing dataset metadata\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets.base:Parsing dataset metadata\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets.base:Found 1000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/imagenet-sample/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/imagenet-sample/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'imagenet-sample'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'imagenet-sample'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 1000/1000 [1.3s elapsed, 0s remaining, 744.3 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 1000/1000 [1.3s elapsed, 0s remaining, 744.3 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'imagenet-sample' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'imagenet-sample' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Welcome to\n",
            "\n",
            "███████╗██╗███████╗████████╗██╗   ██╗ ██████╗ ███╗   ██╗███████╗\n",
            "██╔════╝██║██╔════╝╚══██╔══╝╚██╗ ██╔╝██╔═══██╗████╗  ██║██╔════╝\n",
            "█████╗  ██║█████╗     ██║    ╚████╔╝ ██║   ██║██╔██╗ ██║█████╗\n",
            "██╔══╝  ██║██╔══╝     ██║     ╚██╔╝  ██║   ██║██║╚██╗██║██╔══╝\n",
            "██║     ██║██║        ██║      ██║   ╚██████╔╝██║ ╚████║███████╗\n",
            "╚═╝     ╚═╝╚═╝        ╚═╝      ╚═╝    ╚═════╝ ╚═╝  ╚═══╝╚══════╝ v1.4.1\n",
            "\n",
            "If you're finding FiftyOne helpful, here's how you can get involved:\n",
            "\n",
            "|\n",
            "|  ⭐⭐⭐ Give the project a star on GitHub ⭐⭐⭐\n",
            "|  https://github.com/voxel51/fiftyone\n",
            "|\n",
            "|  🚀🚀🚀 Join the FiftyOne Discord community 🚀🚀🚀\n",
            "|  https://community.voxel51.com/\n",
            "|\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.session.session:\n",
            "Welcome to\n",
            "\n",
            "███████╗██╗███████╗████████╗██╗   ██╗ ██████╗ ███╗   ██╗███████╗\n",
            "██╔════╝██║██╔════╝╚══██╔══╝╚██╗ ██╔╝██╔═══██╗████╗  ██║██╔════╝\n",
            "█████╗  ██║█████╗     ██║    ╚████╔╝ ██║   ██║██╔██╗ ██║█████╗\n",
            "██╔══╝  ██║██╔══╝     ██║     ╚██╔╝  ██║   ██║██║╚██╗██║██╔══╝\n",
            "██║     ██║██║        ██║      ██║   ╚██████╔╝██║ ╚████║███████╗\n",
            "╚═╝     ╚═╝╚═╝        ╚═╝      ╚═╝    ╚═════╝ ╚═╝  ╚═══╝╚══════╝ v1.4.1\n",
            "\n",
            "If you're finding FiftyOne helpful, here's how you can get involved:\n",
            "\n",
            "|\n",
            "|  ⭐⭐⭐ Give the project a star on GitHub ⭐⭐⭐\n",
            "|  https://github.com/voxel51/fiftyone\n",
            "|\n",
            "|  🚀🚀🚀 Join the FiftyOne Discord community 🚀🚀🚀\n",
            "|  https://community.voxel51.com/\n",
            "|\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://5151-gpu-t4-s-2se1w2mmkqen5-c.asia-southeast1-1.prod.colab.dev?polling=true'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "5d53686ea50c84dd"
      },
      "cell_type": "markdown",
      "source": [
        "### FiftyOne Model Zoo\n",
        "\n",
        "The computer vision platform we have been using, FiftyOne, also has a set of models already converted into a format that works with the rest of the FiftyOne platform. Typically, you would have to use library specific code, such as PyTorch, along with other code to specify the architecture to run a computer vision model. With FiftyOne, we can load the model in one line of code,  and then run it for classification (inference) with another line of code. Two lines of code and you are in business.\n",
        "\n",
        "#### ResNet18 in the model zoo\n",
        "\n",
        "We are going to load the PytTorch version of [ResNet18 model](https://docs.voxel51.com/model_zoo/models.html#resnet18-imagenet-torch) that was trained on ImageNet"
      ],
      "id": "5d53686ea50c84dd"
    },
    {
      "metadata": {
        "id": "35c7c114a8295159",
        "outputId": "815784a4-064b-4a1c-d579-a053e8810b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model from 'https://download.pytorch.org/models/resnet18-5c106cde.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.core.models:Downloading model from 'https://download.pytorch.org/models/resnet18-5c106cde.pth'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |████|  357.3Mb/357.3Mb [281.5ms elapsed, 0s remaining, 1.2Gb/s]        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |████|  357.3Mb/357.3Mb [281.5ms elapsed, 0s remaining, 1.2Gb/s]        \n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 132MB/s]\n"
          ]
        }
      ],
      "execution_count": 6,
      "source": [
        "resnet18_imagenet_model = foz.load_zoo_model(\"resnet18-imagenet-torch\")\n"
      ],
      "id": "35c7c114a8295159"
    },
    {
      "metadata": {
        "id": "513b4af3b87017cb"
      },
      "cell_type": "markdown",
      "source": [
        "### Predictions of our Photos\n",
        "\n",
        "We loaded our Flickr dataset and we have loaded our classification model, time to have it predict the classifications for our images."
      ],
      "id": "513b4af3b87017cb"
    },
    {
      "metadata": {
        "id": "a5b1a3f3e26848bd",
        "outputId": "666a0e88-dd7d-47ba-89bc-44118ba952d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 300/300 [8.9s elapsed, 0s remaining, 37.6 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 300/300 [8.9s elapsed, 0s remaining, 37.6 samples/s]      \n"
          ]
        }
      ],
      "execution_count": 21,
      "source": [
        "dataset.apply_model(resnet18_imagenet_model, label_field=\"rn18_in_predictions\", num_workers=12, progress_bar=True)\n",
        "\n",
        "# Now let's look at the results\n",
        "session.dataset = dataset"
      ],
      "id": "a5b1a3f3e26848bd"
    },
    {
      "metadata": {
        "id": "357f29b0d104d977"
      },
      "cell_type": "markdown",
      "source": [
        "#### Deep dive on the horse\n",
        "\n",
        "I want us to dig is on one particular sample\n"
      ],
      "id": "357f29b0d104d977"
    },
    {
      "metadata": {
        "id": "74c3795d61d77fc3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 19,
      "source": [
        "horse_valley = dataset[\"6773012fa08cade6ec7e44f2\"]\n",
        "\n",
        "session.sample_id = horse_valley[\"id\"]"
      ],
      "id": "74c3795d61d77fc3"
    },
    {
      "metadata": {
        "id": "d6fba8ef09ee497c"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's see what the generated predictions tell us"
      ],
      "id": "d6fba8ef09ee497c"
    },
    {
      "metadata": {
        "id": "aff087513d6e0199",
        "outputId": "6141dd9b-f51b-4e74-b592-131160b83352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "torch._VariableFunctionsClass.from_numpy() takes no keyword arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-facfc3feda0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18_imagenet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorse_valley\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rn18_in_predictions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstore_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: torch._VariableFunctionsClass.from_numpy() takes no keyword arguments"
          ]
        }
      ],
      "execution_count": 20,
      "source": [
        "import torch.nn.functional as TF\n",
        "import torch\n",
        "\n",
        "model_classes = resnet18_imagenet_model.classes\n",
        "logits = torch.from_numpy(horse_valley[\"rn18_in_predictions\"][\"logits\"],store_logits=True)\n",
        "\n",
        "\n",
        "print(\"There are \" + str(len(logits))+ \" logits\")\n",
        "\n",
        "print(\"\\nHere are all the logits\")\n",
        "print(str(logits[:25]))\n",
        "\n",
        "confidences = TF.softmax(logits, dim=0)\n",
        "print(\"\\nHere are all the confidence scores\")\n",
        "print(str(confidences[:25]))\n",
        "\n",
        "# Get top 5 values and their indices\n",
        "top_values, top_indices = torch.topk(confidences, k=5)\n",
        "\n",
        "print(\"Top 5 confidence values:\", top_values)\n",
        "print(\"Their indices:\", top_indices)\n",
        "\n",
        "print(\"\\nPredictions in descending confidence:\\n\")\n",
        "for idx, value in zip(top_indices.tolist(), top_values.tolist()):\n",
        "    print(\"Prediction: \" + model_classes[idx] + \" \\tConfidence: \" + str(value))"
      ],
      "id": "aff087513d6e0199"
    },
    {
      "metadata": {
        "id": "4c56f098f03ffad1"
      },
      "cell_type": "markdown",
      "source": [
        "### Discussing the results\n",
        "\n",
        "1. What are some of the main things you noticed about the predictions?\n",
        "2. Were the predicted classes surprising to you? Were they useful for our problem?\n",
        "3. Take home bonus - What did changing the number of workers do?\n",
        "\n",
        "Here are the important ideas I wanted you to take away\n",
        "\n",
        "1. The model only can predict classes it was trained on\n",
        "2. The model will associate the most similar images of its training data to the current image and then give it that class\n"
      ],
      "id": "4c56f098f03ffad1"
    },
    {
      "metadata": {
        "id": "1934b525e27e92b8"
      },
      "cell_type": "markdown",
      "source": [
        "## Another ResNet Model\n",
        "\n",
        "To demonstrate the importance of training data, we are going to run another ResNet18 model, except I trained this model on [Pokemon images](https://huggingface.co/datasets/TheSteve0/pokemon).\n",
        "\n",
        "I put the model weights file in our shared drive.\n",
        "\n",
        "To use this model we are going to:\n",
        "1. Load the model into pytorch\n",
        "2. Run the model against our Flickr images\n",
        "3. Associate the classification labels back to our FiftyOne dataset\n",
        "4. View the results"
      ],
      "id": "1934b525e27e92b8"
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "id": "4d0c4709340ef662"
      },
      "cell_type": "code",
      "source": [
        "pokemon_class_labels = {0: \"Abra\", 1: \"Aerodactyl\", 2: \"Alakazam\", 3: \"Alolan Sandslash\", 4: \"Arbok\", 5: \"Arcanine\", 6: \"Articuno\", 7: \"Beedrill\", 8: \"Bellsprout\", 9: \"Blastoise\", 10: \"Bulbasaur\", 11: \"Butterfree\", 12: \"Caterpie\", 13: \"Chansey\", 14: \"Charizard\", 15: \"Charmander\", 16: \"Charmeleon\", 17: \"Clefable\", 18: \"Clefairy\", 19: \"Cloyster\", 20: \"Cubone\", 21: \"Dewgong\", 22: \"Diglett\", 23: \"Ditto\", 24: \"Dodrio\", 25: \"Doduo\", 26: \"Dragonair\", 27: \"Dragonite\", 28: \"Dratini\", 29: \"Drowzee\", 30: \"Dugtrio\", 31: \"Eevee\", 32: \"Ekans\", 33: \"Electabuzz\", 34: \"Electrode\", 35: \"Exeggcute\", 36: \"Exeggutor\", 37: \"Farfetchd\", 38: \"Fearow\", 39: \"Flareon\", 40: \"Gastly\", 41: \"Gengar\", 42: \"Geodude\", 43: \"Gloom\", 44: \"Golbat\", 45: \"Goldeen\", 46: \"Golduck\", 47: \"Golem\", 48: \"Graveler\", 49: \"Grimer\", 50: \"Growlithe\", 51: \"Gyarados\", 52: \"Haunter\", 53: \"Hitmonchan\", 54: \"Hitmonlee\", 55: \"Horsea\", 56: \"Hypno\", 57: \"Ivysaur\", 58: \"Jigglypuff\", 59: \"Jolteon\", 60: \"Jynx\", 61: \"Kabuto\", 62: \"Kabutops\", 63: \"Kadabra\", 64: \"Kakuna\", 65: \"Kangaskhan\", 66: \"Kingler\", 67: \"Koffing\", 68: \"Krabby\", 69: \"Lapras\", 70: \"Lickitung\", 71: \"Machamp\", 72: \"Machoke\", 73: \"Machop\", 74: \"Magikarp\", 75: \"Magmar\", 76: \"Magnemite\", 77: \"Magneton\", 78: \"Mankey\", 79: \"Marowak\", 80: \"Meowth\", 81: \"Metapod\", 82: \"Mew\", 83: \"Mewtwo\", 84: \"Moltres\", 85: \"MrMime\", 86: \"Muk\", 87: \"Nidoking\", 88: \"Nidoqueen\", 89: \"Nidorina\", 90: \"Nidorino\", 91: \"Ninetales\", 92: \"Oddish\", 93: \"Omanyte\", 94: \"Omastar\", 95: \"Onix\", 96: \"Paras\", 97: \"Parasect\", 98: \"Persian\", 99: \"Pidgeot\", 100: \"Pidgeotto\", 101: \"Pidgey\", 102: \"Pikachu\", 103: \"Pinsir\", 104: \"Poliwag\", 105: \"Poliwhirl\", 106: \"Poliwrath\", 107: \"Wigglytuff\", 108: \"Zapdos\", 109: \"Zubat\"}\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.v2 as T\n",
        "from PIL import Image\n",
        "import fiftyone as fo\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.amp as amp\n",
        "\n",
        "# Enable CUDA optimization\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# if gpu available use that, otherwise use cpu\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model using pickle\n",
        "with open('/content/drive/MyDrive/impatient-cv/pokemon-classification-model.pt', 'rb') as f:\n",
        "    state_dict = pickle.load(f)\n",
        "\n",
        "# Create a ResNet18 model with no pre-trained weights\n",
        "model = models.resnet18(weights=None)\n",
        "\n",
        "# Modify the final layer to match the trained model's 150 output classes\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 150)\n",
        "\n",
        "# Check if keys have the nested prefix and remove it\n",
        "if any(k.startswith('model.model.') for k in state_dict.keys()):\n",
        "    new_state_dict = {k.replace('model.model.', ''): v for k, v in state_dict.items()}\n",
        "    state_dict = new_state_dict\n",
        "elif any(k.startswith('model.') for k in state_dict.keys()):\n",
        "    new_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
        "    state_dict = new_state_dict\n",
        "\n",
        "# Load the state dict into the model\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# pre-processing transforms we did for model training\n",
        "transform = T.Compose([\n",
        "    T.ToImage(),\n",
        "    T.RGB(),\n",
        "    T.ToDtype(torch.float32, scale=True),\n",
        "    T.Resize(224),\n",
        "    T.CenterCrop(224),\n",
        "])\n",
        "\n",
        "# Load your FiftyOne dataset is already done in the notebook\n",
        "\n",
        "# Optional: Define class mapping for Pokemon species (if available)\n",
        "# class_names = {0: \"Pikachu\", 1: \"Charizard\", ...}\n",
        "class_names = pokemon_class_labels  # Set to None if unavailable\n",
        "\n",
        "# Custom dataset for parallel loading\n",
        "class PokemonDataset(Dataset):\n",
        "    def __init__(self, sample_ids, filepaths, transform=None):\n",
        "        self.sample_ids = sample_ids\n",
        "        self.filepaths = filepaths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filepaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.sample_ids[idx]\n",
        "        filepath = self.filepaths[idx]\n",
        "\n",
        "        image = Image.open(filepath).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return sample_id, image\n",
        "\n",
        "# Extract sample IDs and filepaths\n",
        "sample_ids = dataset.values(\"id\")\n",
        "filepaths = dataset.values(\"filepath\")\n",
        "\n",
        "# Create dataset and dataloader for parallel processing\n",
        "pokemon_dataset = PokemonDataset(sample_ids, filepaths, transform)\n",
        "dataloader = DataLoader(\n",
        "    pokemon_dataset,\n",
        "    batch_size=64,  # Larger batch size for GPU efficiency\n",
        "    num_workers=2,  # Reduced worker count to avoid warnings\n",
        "    pin_memory=True  # Faster data transfer to GPU\n",
        ")\n",
        "\n",
        "# Prepare for mixed precision if supported\n",
        "scaler = amp.GradScaler(enabled=True)\n",
        "\n",
        "# Create a list to store FiftyOne Classification objects\n",
        "all_classifications = [None] * len(sample_ids)\n",
        "\n",
        "# Process in batches\n",
        "with torch.no_grad():\n",
        "    for batch_ids, images in tqdm(dataloader):\n",
        "        # Move images to device\n",
        "        images = images.to(device, non_blocking=True)\n",
        "\n",
        "        # Run inference with mixed precision\n",
        "        with amp.autocast('cuda', enabled=True):\n",
        "            outputs = model(images)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "        # Get results from GPU\n",
        "        predictions = predictions.cpu().numpy()\n",
        "        confidences = confidences.cpu().numpy()\n",
        "\n",
        "        # Store results as FiftyOne Classification objects\n",
        "        for i, sample_id in enumerate(batch_ids):\n",
        "            # Find index in the original arrays\n",
        "            idx = sample_ids.index(sample_id)\n",
        "\n",
        "            pred_idx = int(predictions[i])\n",
        "            confidence = float(confidences[i])\n",
        "\n",
        "            # Get class name if mapping exists\n",
        "            if class_names is not None:\n",
        "                pred_label = class_names.get(pred_idx, f\"Unknown({pred_idx})\")\n",
        "            else:\n",
        "                # If no class names mapping, use stringified index as label\n",
        "                pred_label = str(pred_idx)\n",
        "\n",
        "            # Create a FiftyOne Classification object\n",
        "            classification = fo.Classification(\n",
        "                label=pred_label,\n",
        "                confidence=confidence\n",
        "            )\n",
        "\n",
        "            all_classifications[idx] = classification\n",
        "\n",
        "# Use set_values to update all samples with classification objects in a single batch operation\n",
        "dataset.set_values(\"pokemon_classification\", all_classifications)\n",
        "\n",
        "# Save dataset\n",
        "dataset.save()\n",
        "\n",
        "print(\"\\nFinished classifying\\n\\n\")\n",
        "\n",
        "session = fo.launch_app(dataset, auto=False)\n",
        "session.url"
      ],
      "id": "4d0c4709340ef662",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "54766fccb120fb44"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrap up\n",
        "\n",
        "And with that we are done with classification. In the next notebook, we are going to reuse these models to demonstrate the importance of data when doing embeddings\n",
        "\n",
        "[Embdeddings](https://github.com/thesteve0/impatient-computer-vision/blob/main/3_embeddings.ipynb)"
      ],
      "id": "54766fccb120fb44"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}